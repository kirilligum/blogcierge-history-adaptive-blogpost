---
import { getCollection, getEntryBySlug } from "astro:content";
import BlogPostLayout from "../../layouts/BlogPostLayout.astro"; // Ensure this path is correct
import type { KVNamespace } from "@cloudflare/workers-types";

// --- Helper functions & Constants ---
const TAILORED_CONTENT_CACHE_TTL_SECONDS = 60 * 60 * 24; // 1 day

function getApiKey(locals: Astro.Locals, devMode: boolean): string | undefined {
  if (locals.runtime?.env?.LLAMA_API_KEY) {
    return locals.runtime.env.LLAMA_API_KEY;
  }
  if (devMode && import.meta.env.LLAMA_API_KEY) {
    return import.meta.env.LLAMA_API_KEY;
  }
  console.warn("[SSR Tailoring] LLAMA_API_KEY not found.");
  return undefined;
}

// Replace with your actual LLM API URL and desired model
const LLAMA_API_URL = "https://api.llama.com/v1/chat/completions"; 
const DEFAULT_MODEL = "Llama-4-Maverick-17B-128E-Instruct-FP8"; 

async function getUserInteractionHistory(kv: KVNamespace, deviceId: string): Promise<{ readPostSlugs: string[], chatMessages: any[] }> {
  const readPostSlugsSet = new Set<string>();
  const chatMessages: any[] = [];
  
  // KV Key structure: deviceId/YYYY-MM-DD/slug
  // Data structure: { read: boolean, messages: Array<{role: string, content: string, timestamp: string}> }
  const { keys } = await kv.list({ prefix: `${deviceId}/` });

  for (const key of keys) {
    try {
      const data = await kv.get<any>(key.name, { type: "json" });
      if (data) {
        const parts = key.name.split('/'); 
        if (parts.length === 3 && data.read === true && parts[2]) {
          readPostSlugsSet.add(parts[2]);
        }
        if (data.messages && Array.isArray(data.messages)) {
          // Take last 5 message pairs (10 messages total) to manage context size. Adjust as needed.
          chatMessages.push(...data.messages.slice(-10)); 
        }
      }
    } catch (e) {
      console.error(`[SSR Tailoring] Error parsing KV data for key ${key.name}:`, e);
    }
  }
  return { readPostSlugs: Array.from(readPostSlugsSet), chatMessages };
}
// --- End Helper functions & Constants ---

export const prerender = false; // Enable SSR for this page

export async function getStaticPaths() {
  const blogEntries = await getCollection("blog");
  return blogEntries.map((entry) => ({
    params: { slug: entry.slug },
    // props: { entry }, // Props from getStaticPaths are not directly passed to Astro.props in SSR for dynamic routes.
                       // The entry is fetched below using Astro.params.slug.
  }));
}

const { slug } = Astro.params;

if (typeof slug !== 'string') {
  // This should ideally not be reached if the route pattern is matched correctly by Astro.
  console.error("[SSR Blog Page] Error: Slug is undefined or not a string from Astro.params.");
  // Return a 400 Bad Request response as the slug is essential for this page.
  return new Response("Invalid request: Slug parameter is missing or invalid.", { status: 400 });
}

const entry = await getEntryBySlug("blog", slug);

if (!entry) {
  // If no entry is found for the given slug, return a 404 Not Found response.
  console.warn(`[SSR Blog Page] No blog entry found for slug: ${slug}`);
  return new Response(null, { status: 404, statusText: "Not Found" });
}

// OriginalContentComponent is used as a fallback if personalization fails or is not applicable
const { Content: OriginalContentComponent } = await entry.render();

let finalContentToRender: string | null = null; // This will hold the personalized Markdown string
let contentSourceDebug = "original_static_fallback_ssr"; // For debugging the content source

const deviceId = Astro.cookies.get("blgcUserDeviceID_cookie")?.value;

if (deviceId) {
  const userInteractionsKV = Astro.locals.runtime?.env?.BLGC_USER_INTERACTIONS_KV as KVNamespace | undefined;
  const tailoredContentCache = Astro.locals.runtime?.env?.BLGC_BLOGPOST_AI_CACHE as KVNamespace | undefined;
  const currentPostSlug = entry.slug;
  const cacheKey = `tailored_ssr::${currentPostSlug}::${deviceId}`;

  if (tailoredContentCache) {
    try {
      const cachedContent = await tailoredContentCache.get(cacheKey);
      if (cachedContent) {
        finalContentToRender = cachedContent;
        contentSourceDebug = "cache_ssr";
        console.log(`[SSR Tailoring] Cache HIT for ${cacheKey}`);
      } else {
        console.log(`[SSR Tailoring] Cache MISS for ${cacheKey}`);
      }
    } catch (e) {
      console.error(`[SSR Tailoring] Error reading from tailoredContentCache for ${cacheKey}:`, e);
    }
  }

  // If not found in cache, and KV for user interactions is available
  if (!finalContentToRender && userInteractionsKV) {
    const { readPostSlugs, chatMessages } = await getUserInteractionHistory(userInteractionsKV, deviceId);

    // Only attempt LLM call if there's some history to personalize with
    if (readPostSlugs.length > 0 || chatMessages.length > 0) {
      let readPostsCombinedContent = "";
      for (const readSlug of readPostSlugs) {
        if (readSlug === currentPostSlug) continue; // Don't include current post in "previously read"
        try {
          const readEntry = await getEntryBySlug("blog", readSlug);
          if (readEntry) {
            // Truncate for context window management. Consider summarization for production.
            readPostsCombinedContent += `\n\n--- BEGIN READ BLOG POST: ${readSlug} (Title: ${readEntry.data.title}) ---\n${readEntry.body.substring(0, 2000)}...\n--- END READ BLOG POST: ${readSlug} ---`;
          }
        } catch (e) { 
          console.warn(`[SSR Tailoring] Could not fetch content for read post ${readSlug}:`, e); 
        }
      }

      // Truncate chat messages for context window management
      const chatHistoryString = chatMessages.map(msg => `${msg.role}: ${msg.content.substring(0, 200)}...`).join('\n');
      const originalPostContentForLLM = entry.body; // Full original Markdown body of the current post

      const systemPrompt = `
You are an expert blog post author. Rewrite the following "ORIGINAL BLOG POST CONTENT" for a reader with the provided "READER'S HISTORY".

READER'S HISTORY (Device ID: ${deviceId}):
--- BEGIN READER'S READ BLOG POSTS (Excerpts/Summaries) ---
${readPostsCombinedContent || "No other posts read by the user."}
--- END READER'S READ BLOG POSTS ---

--- BEGIN READER'S AI CHAT HISTORY (Recent/Truncated messages) ---
${chatHistoryString || "No chat history with AI assistant."}
--- END READER'S AI CHAT HISTORY ---

ORIGINAL BLOG POST CONTENT (Slug: ${currentPostSlug}, Title: ${entry.data.title}):
--- BEGIN ORIGINAL CONTENT ---
${originalPostContentForLLM}
--- END ORIGINAL CONTENT ---

TASK:
Rewrite the "ORIGINAL BLOG POST CONTENT" as Markdown.
1. If concepts from the original post are well-covered in "READER'S READ BLOG POSTS" or the reader showed understanding in "READER'S AI CHAT HISTORY", summarize them very briefly or omit if redundant.
2. If concepts are new OR if the "READER'S AI CHAT HISTORY" indicates confusion, questions about related topics, or a desire to learn more about something relevant to the current post, expand on these concepts. Make clear connections to what the user asked about or previously read if relevant.
3. Maintain the core message, factual accuracy, and overall tone of the original post.
4. Output *only* the rewritten Markdown content for the blog post body. Do not include any frontmatter or titles unless it's part of the markdown body itself (e.g. H1 for title).
If the history is minimal or uninformative for personalization, the rewritten content should be very similar to the original, perhaps with slightly more explanation for basic terms.
The goal is to make the post maximally informative and engaging for *this specific reader*.
Focus on clarity and conciseness. The output should be ready-to-render Markdown.
`;
          const apiKey = getApiKey(Astro.locals, import.meta.env.DEV);
          if (apiKey) {
            const payload = {
              model: DEFAULT_MODEL,
              messages: [{ role: "system", content: systemPrompt }],
              max_tokens: 4000, // Adjust based on expected output length and model capabilities
              temperature: 0.5, // Adjust for creativity vs. factuality
            };
            try {
              console.log(`[SSR Tailoring] Calling LLM for ${currentPostSlug}, user ${deviceId}. Model: ${DEFAULT_MODEL}`);
              const llmResponse = await fetch(LLAMA_API_URL, {
                method: "POST",
                headers: { "Content-Type": "application/json", Authorization: `Bearer ${apiKey}` },
                body: JSON.stringify(payload),
              });

              if (llmResponse.ok) {
                const llmData = await llmResponse.json();
                // Adjust based on your LLM's actual response structure
                const llmGeneratedMarkdown = llmData.completion_message?.content?.text || llmData.choices?.[0]?.message?.content; 

                if (llmGeneratedMarkdown && llmGeneratedMarkdown.trim() !== "") {
                  finalContentToRender = llmGeneratedMarkdown;
                  contentSourceDebug = "llm_generated_ssr";
                  console.log(`[SSR Tailoring] LLM generated content for ${currentPostSlug}, user ${deviceId}.`);
                  if (tailoredContentCache) {
                    Astro.locals.runtime.ctx.waitUntil(
                      tailoredContentCache.put(cacheKey, finalContentToRender, { expirationTtl: TAILORED_CONTENT_CACHE_TTL_SECONDS })
                        .then(() => console.log(`[SSR Tailoring] Successfully cached tailored content for ${cacheKey}`))
                        .catch(e => console.error(`[SSR Tailoring] Error caching tailored content for ${cacheKey}:`, e))
                    );
                  }
                } else {
                  contentSourceDebug = "llm_error_empty_content_ssr";
                  console.warn(`[SSR Tailoring] LLM returned empty content for ${currentPostSlug}, user ${deviceId}.`);
                }
              } else {
                contentSourceDebug = `llm_error_api_status_${llmResponse.status}_ssr`;
                console.error(`[SSR Tailoring] LLM API error for ${currentPostSlug}, user ${deviceId}: Status ${llmResponse.status}`, await llmResponse.text());
              }
            } catch (error) {
              contentSourceDebug = "llm_error_fetch_exception_ssr";
              console.error(`[SSR Tailoring] LLM fetch exception for ${currentPostSlug}, user ${deviceId}:`, error);
            }
          } else {
            contentSourceDebug = "original_ssr_no_apikey";
          }
        } else {
          contentSourceDebug = "original_ssr_no_history";
          console.log(`[SSR Tailoring] No significant interaction history for ${deviceId} on ${currentPostSlug}. Serving original content.`);
        }
      } else if (!deviceId) {
        contentSourceDebug = "original_ssr_no_cookie";
      } else {
        // This case implies userInteractionsKV was not available.
        contentSourceDebug = "original_ssr_no_kv_access";
        console.warn(`[SSR Tailoring] User Interactions KV not available. Serving original content for ${currentPostSlug}.`);
      }
    } else if (!deviceId) {
        contentSourceDebug = "original_ssr_no_cookie";
        console.log(`[SSR Tailoring] No deviceId cookie found. Serving original content for ${currentPostSlug}.`);
    }
    // If finalContentToRender is still null at this point, the OriginalContentComponent will be used.

// Remove processedMarkdownHtml and its associated logic.
// Astro.markdown.render will be called directly in the template if finalContentToRender is a valid string.

console.log(`[SSR Blog Post] Slug: ${entry.slug}, DeviceID: ${deviceId || 'N/A'}, Source: ${contentSourceDebug}`);
---

<BlogPostLayout frontmatter={entry.data} slug={entry.slug}>
  <div class="post-body">
    {typeof finalContentToRender === 'string' && finalContentToRender.trim() !== '' ? (
      <Fragment set:html={Astro.markdown.render(finalContentToRender)} />
    ) : (
      <OriginalContentComponent />
    )}
    {/* For debugging, you can uncomment this to see the content source on the page: */}
    {/* <p style="font-size: 0.8em; color: grey; margin-top: 2em; text-align: center; background: #f0f0f0; padding: 5px;">Content Source: {contentSourceDebug}</p> */}
  </div>
</BlogPostLayout>
